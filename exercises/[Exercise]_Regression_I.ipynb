{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Exercise] Regression I.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv49axWeylsB"
      },
      "source": [
        "This notebook is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/).\n",
        "\n",
        "Author: 蘇嘉冠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0_QjevL_F5"
      },
      "source": [
        "# [Exercise 2] Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75HmxuUENW8"
      },
      "source": [
        "## 展示題：房價預測（單變數版本）\n",
        "\n",
        "我們蒐集到了波士頓郊區的房價資料集（[來源](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/?C=N;O=D)），想要從某城鎮的一個 feature，來預測該城鎮自用住宅的房價中位數（`MEDV`）。\n",
        "\n",
        "我們將這個資料集的 csv 檔讀入至一個 pandas 的 DataFrame：`df`。資料的各個 column 的意義如下：\n",
        "- `CRIM`：某城鎮的人均犯罪率\n",
        "- `ZN`：「超過 25,000 平方呎的住宅用地區塊」所佔的比例\n",
        "- `INDUS`：某城鎮「非零售的商業用地」比例（英畝）\n",
        "- `NOX`：一氧化氮濃度（以 10 ppm 為單位）\n",
        "- `RM`：平均每戶有幾個房間\n",
        "- `AGE`： 1940 年之前所建的房屋，屋主自用的比例\n",
        "- `DIS`：到波士頓五個就業服務中心的（加權）距離\n",
        "- `RAD`：使用高速公路的方便性 / 可達性指數\n",
        "- `TAX`：「總價 / 房屋稅」的比例（單位：10,000 美金）\n",
        "- `PTRATIO`：某城鎮的「生 / 師」比\n",
        "- `LSTAT`：低所得的人口比例\n",
        "- `MEDV`：自用住宅的房價中位數（單位：1,000 美金）\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJA285HdE5sV"
      },
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn mlxtend==0.18.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-rM4vVFkR7y"
      },
      "source": [
        "### Exploratory Data Analysis (EDA)\n",
        "\n",
        "在開使訓練一個機器學習模型之前，一個很重要的工作是做 Exploratory Data Analysis (EDA)，透過視覺化的方式來了解資料的分佈、feature 之間的關係等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qFffL07R8k5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/AINTUT/code_2022/main/datasets/\"\n",
        "    \"house_pricing.csv\",\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "DX_LU2A6NxzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(df))"
      ],
      "metadata": {
        "id": "YaeBtWOzOGmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jId3MMWbaU-w"
      },
      "source": [
        "第一步是用各個 feature 之間的分佈來看 feature 之間的關係。在這裡我們使用 [mlxtend](https://rasbt.github.io/mlxtend/) 這個 package，並且選了幾個 feature 來做視覺化，可以嘗試看看修改 `columns` 的內容來看不同的 feature，或是將 `columns` 改成 `columns = list(df)` 來看所有 feature 之間的關係。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNj-jA_kmAI8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import scatterplotmatrix\n",
        "\n",
        "columns = [\n",
        "    \"CRIM\",\n",
        "    \"NOX\",\n",
        "    \"RM\",\n",
        "    \"DIS\",\n",
        "    \"TAX\",\n",
        "    \"PTRATIO\",\n",
        "    \"MEDV\",\n",
        "]\n",
        "\n",
        "scatterplotmatrix(\n",
        "    df[columns].to_numpy(),\n",
        "    figsize=(10, 8),\n",
        "    names=columns,\n",
        "    alpha=0.5,\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UO3yoowb6G9"
      },
      "source": [
        "第二步是去算各個 feature 之間的 [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)，並且用熱力圖（heatmap）來做呈現。這個數值如果接近 1 代表某兩個 feature 有很強的正關係，接近 -1 代表有很強的負關係，接近 0 則代表關係不大。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCrBim6joi1T"
      },
      "source": [
        "import numpy as np\n",
        "from mlxtend.plotting import heatmap\n",
        "\n",
        "cm = np.corrcoef(df[columns].to_numpy().T)\n",
        "hm = heatmap(cm, row_names=columns, column_names=columns)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIVKoX2ctrt"
      },
      "source": [
        "做完以上的 EDA 之後，我們認為 `RM` 這個變數跟我們的目標：房價（`MEDV`）很有關係，所以將這兩者之間的散佈圖畫出來一下，並且將 `RM` 作為 input feature（`x_data`），`MEDV` 作為預測的目標（`y_data`）。\n",
        "\n",
        "附註：\n",
        "- `x_data` 的 shape 為 (506, 1)，前面的數字代表有 506 筆資料，後面的 1 則代表有一個 input feature。之後我們學到多個 input feature 時，後面的數字會大於 1。\n",
        "- `y_data` 的 shape (506,)，只有一個數字，因為每筆資料對應到的目標數值永遠只有一個，所以不需要再額外多一個維度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELkhwY1TYrO"
      },
      "source": [
        "df.plot.scatter(x=\"RM\", y=\"MEDV\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0yNyZjzsAAt"
      },
      "source": [
        "x_data = df[[\"RM\"]].to_numpy()\n",
        "y_data = df[\"MEDV\"].to_numpy()\n",
        "\n",
        "print(x_data.shape)\n",
        "print(y_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_0lLZuiq0_w"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "選定 feature 後，我們將對資料做兩個步驟的前處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHnIeJcPdkJ7"
      },
      "source": [
        "第一步，使用 [scikit-learn](https://scikit-learn.org/stable/index.html) 的 [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)，將資料分割成 training data 與 testing data，比例為 80:20。\n",
        "\n",
        "附註：\n",
        "- `x_train` 與 `y_train` 代表 training data\n",
        "- `x_test` 與 `y_test` 代表 testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHbYlPgsqq6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data,\n",
        "    y_data,\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3F-M5SduT2"
      },
      "source": [
        "第二步，使用 scikit-learn 的 [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)，以 standardization 的方法對資料做 feature scaling。在做 standardization 需要給一批資料來算這批資料的平均值與標準差，然後在實際的場景中，我們並不會知道 testing data 的平均值與標準差。因此我們的作法是：先算出 training data 的平均值與標準差，將之同時用在 training 與 testing data 上。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioEQ8QANVy6J"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train_std = scaler.fit_transform(x_train)\n",
        "x_test_std = scaler.transform(x_test)\n",
        "\n",
        "print(\"Before standardization:\")\n",
        "print(x_train[:10])\n",
        "print(\"After standardization:\")\n",
        "print(x_train_std[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83AzyzUnvAoS"
      },
      "source": [
        "### Training\n",
        "\n",
        "附註：\n",
        "\n",
        "這個段落比較困難，數學的部份可以跳過，有興趣挑戰的同學再來看細節。因為目前很多工具都有實做類似的功能，以應用的角度來說只要知道主要觀念（訓練的三個大步驟）以及如何使用就好。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "開始我們訓練模型的流程，主要包含幾個步驟：\n",
        "1. 定義 function set：$f(x) = b + w_{1}x_{1}$\n",
        "2. 定義 loss function：$L(b, w_{1}) = \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y}^{(i)} - (b + w_{1}{x}^{(i)}_{1}))^2$\n",
        "3. 學習，使用 Gradient Descent 來找最佳的 function：$f^{*}$"
      ],
      "metadata": {
        "id": "TtzUHXH2ebcb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTFdyVAYfgIH"
      },
      "source": [
        "用 `predict()` 這個 function 來實現 $f(x)$ 的計算過程，其中：\n",
        "- `x` 代表某 n 筆房屋的資料，型別為 `numpy.ndarray`，如果每筆資料有 k 個 input feature，則 `x` 的 shape 為 (n, k) （目前我們只有考慮 `RM` 這個 feature，也就是 k = 1，而 `predict()` 可以同時處理 k > 1 的情況 ）\n",
        "- `weights` 代表權重，型別為 `numpy.ndarray`。目前我們的權重會長的像 $[b, w_{1}]$ 這樣，`predict()` 可以處理擴展到 k + 1 個權重的狀況（$[b, w_1, w_2, ..., w_k]$）\n",
        "\n",
        "根據 `x` 跟 `weights`，我們可以算出相對應的 $f(x)$ 。例如我們 k = 1，有 3 筆資料的時候，我們可以算出這 3 筆資料相對應的結果：\n",
        "$f(x) = \\begin{bmatrix}x^{(1)}_{1} \\\\ x^{(2)}_1 \\\\ x^{(3)}_{1} \\end{bmatrix}\\dot{}\\begin{bmatrix}w_{1}\\end{bmatrix} + b = \\begin{bmatrix}w_{1}x^{(1)}_{1} \\\\ w_{1}x^{(2)}_1 \\\\ w_{1}x^{(3)}_{1} \\end{bmatrix} + b = \\begin{bmatrix}w_{1}x^{(1)}_{1} + b \\\\ w_{1}x^{(2)}_1 + b \\\\ w_{1}x^{(3)}_{1} + b \\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Pox8MGxq7_"
      },
      "source": [
        "def predict(x, weights):\n",
        "    return np.dot(x, weights[1:]) + weights[0]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WGY9ZMqGbw"
      },
      "source": [
        "用 `calculate_loss()` 這個 function 計算 loss：$L(b, w_{1})$（SSE, Sum of Squared Error），其中：\n",
        "- `y_gt`：代表某 n 筆房屋真實的房價，型別為 `numpy.ndarray`，shape 為 (n,)\n",
        "- `y_pred`：代表某 n 筆房屋預測的房價，型別為 `numpy.ndarray`，shape 為 (n,)\n",
        "\n",
        "根據 `y_gt` 與 `y_pred`，來算出 loss 的數值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vRU307AzgeR"
      },
      "source": [
        "def calculate_loss(y_gt, y_pred):\n",
        "    loss = ((y_gt - y_pred) ** 2).sum() / 2.0\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHfLwuXPr7Ba"
      },
      "source": [
        "`fit()` 這個 function 用來跑訓練的流程：\n",
        "1. 初始化 `weights`，數值皆為 0（我們只有 1 個 feature，所以 `weights` 長的像這樣：$[0, 0]$）\n",
        "2. 做 gradient descent：先根據目前的 `weights` 來計算 `f(x)`，再計算 `weights` 中每個值對 loss 的偏微分，並且用偏微分的結果來更新 `weights`（更新的幅度由 `learning_rate` 來決定）\n",
        "3. 計算 loss 並且記錄下來\n",
        "4. 重複步驟 2 ~ 3 好幾個 epoch（由 `epoches` 來決定要重複幾次）\n",
        "5. 訓練完成後，將訓練好的 `weights`，以及每個 epoch 紀錄下來的 loss 回傳回去\n",
        "\n",
        "關於偏微分請參考課程的[簡報](https://docs.google.com/presentation/d/1Dntz67pZ_mbShQZhkyO9llqcx7pv7Pbs_R67Lf-5UGQ/edit#slide=id.gcd0121f0ef_0_1)。`fit()` 可以處理有 k 個 feature 的情況，而簡報是在說 k = 1，也就是我們目前資料的狀況。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIJLbj3HzioA"
      },
      "source": [
        "def fit(x_train, y_train, epoches, learning_rate):\n",
        "    weights = np.zeros(x_train.shape[1] + 1)\n",
        "    losses = []\n",
        "\n",
        "    for _ in range(epoches):\n",
        "        y_pred = predict(x_train, weights)\n",
        "\n",
        "        diff = y_train - y_pred\n",
        "        weights[0] = weights[0] - learning_rate * -diff.sum()\n",
        "        weights[1:] = weights[1:] - learning_rate * -x_train.T.dot(diff)\n",
        "\n",
        "        losses.append(calculate_loss(y_train, y_pred))\n",
        "\n",
        "    return weights, losses"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jh-tFM6187X"
      },
      "source": [
        "開始跑我們的訓練，設定好 `epoches` 以及 `learning_rate` 這兩個 hyper parameter，並且將訓練資料與 hyper parameters 丟進去 `fit()`，得到訓練好的 `weights` 以及 `losses`（每個 epoch 紀錄下來的 loss）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPqfCd-XVfhi"
      },
      "source": [
        "epoches = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "weights, losses = fit(x_train_std, y_train, epoches, learning_rate)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uX-D2HH2nzj"
      },
      "source": [
        "我們將每個 epoch 的 loss 的圖畫出來，來確認 loss 是否有逐漸下降！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld8BmR8LVr1m"
      },
      "source": [
        "plt.plot(range(1, epoches + 1), losses)\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snCXX6gTk45j"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "來看一下我們的訓練成果！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmUvFSKp3Cv3"
      },
      "source": [
        "這裡的 `reg_plot()` 將以下資訊同時畫在同張圖：\n",
        "1. 每筆房屋資料，input feature 與真實房價之間的散佈圖\n",
        "2. 每筆房屋資料，input feature 與預測房價之間的曲線圖"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ZsyOBjjd9E"
      },
      "source": [
        "def reg_plot(x, y_gt, y_pred):\n",
        "    plt.scatter(x, y_gt, c=\"steelblue\", edgecolor=\"white\")\n",
        "    plt.plot(x, y_pred, c=\"black\")\n",
        "\n",
        "    plt.xlabel(\"RM\")\n",
        "    plt.ylabel(\"MEDV\")\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-p_ZCGq3ysY"
      },
      "source": [
        "對於 training data，先用 `reg_plot()` 畫出結果的圖來，再用 scikit-learn 的 [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html?highlight=mean_squared_error#sklearn.metrics.mean_squared_error) 算 Mean Squared Error（MSE）的數值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll7-tGlQjmMw"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = predict(x_train_std, weights)\n",
        "\n",
        "reg_plot(x_train, y_train, y_pred)\n",
        "\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "\n",
        "print(\"MSE of Training Data: {}\".format(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap4QIgxO4Af7"
      },
      "source": [
        "對於 testing data，先用 `reg_plot()` 畫出結果的圖來，再算 Mean Squared Error（MSE）的數值。\n",
        "\n",
        "附註：一般而言，testing data 的 MSE 會比 training 的還大"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQPNT5rKqDw"
      },
      "source": [
        "y_pred = predict(x_test_std, weights)\n",
        "\n",
        "reg_plot(x_test, y_test, y_pred)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"MSE of Testing Data: {}\".format(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG2emDCCk_Fh"
      },
      "source": [
        "### 改用 Scikit-Learn 做 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6bvKPVj6Mq5"
      },
      "source": [
        "如果不想要實做上面複雜的 training 流程，我們可以改用 scikit-learn 的 [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression) 來實做，可以在很少行 code 的狀況下完成一個 linear regression 的訓練。不過要注意的是 scikit-learn 並不是用 gradient descent 來實做（[參考](https://stackoverflow.com/questions/34469237/linear-regression-and-gradient-descent-in-scikit-learn)），而且不用對資料做 feature scaling。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEGf4fzalFRp"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH_fGdEbnykI"
      },
      "source": [
        "y_pred = lr.predict(x_train)\n",
        "\n",
        "reg_plot(x_train, y_train, y_pred)\n",
        "\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "\n",
        "print(\"MSE of Training Data: {}\".format(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kth7pi9XoDJO"
      },
      "source": [
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "reg_plot(x_test, y_test, y_pred)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"MSE of Testing Data: {}\".format(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GILStZ3CBSAV"
      },
      "source": [
        "## 練習題\n",
        "\n",
        "1. 我們想改用 `LSTAT` 作為 input feature，來預測房價（`MEDV`）。請修改上面的程式碼來達成我們的目標，並觀察 training 與 testing data 的 MSE 為多少。\n",
        "2. 我們想改用 `CRIM` 作為 input feature，來預測房價（`MEDV`）。請修改上面的程式碼來達成我們的目標，並觀察 training 與 testing data 的 MSE 為多少。\n",
        "3. 上面兩者的 MSE，何者比較低？造成這種結果的原因是什麼"
      ]
    }
  ]
}